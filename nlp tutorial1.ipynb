{
 "cells": [
  {
   "cell_type": "raw",
   "id": "86497c17-5552-4d41-85b3-bdf1558aa68a",
   "metadata": {},
   "source": [
    "TUTORIAL 1\n",
    "S SARAH \n",
    "RA2211042010034\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "687fd0c0-9a3b-4ece-974b-94f768d9a345",
   "metadata": {},
   "source": [
    "TOKENIZATION-LEMMA-STEM-STOP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce3f52b-dfad-402b-b1b2-f3b80d54dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "640eae27-16b5-46c6-a384-d387a55ccb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Shape: (8551, 8)\n",
      "Column Names: Index(['Unnamed: 0', 'id', 'title', 'overview', 'release_date', 'popularity',\n",
      "       'vote_average', 'vote_count'],\n",
      "      dtype='object')\n",
      "First 5 Rows:\n",
      "    Unnamed: 0      id                        title  \\\n",
      "0           0   19404  Dilwale Dulhania Le Jayenge   \n",
      "1           1  724089    Gabriel's Inferno Part II   \n",
      "2           2     278     The Shawshank Redemption   \n",
      "3           3     238                The Godfather   \n",
      "4           4  761053   Gabriel's Inferno Part III   \n",
      "\n",
      "                                            overview release_date  popularity  \\\n",
      "0  Raj is a rich, carefree, happy-go-lucky second...   20-10-1995      18.433   \n",
      "1  Professor Gabriel Emerson finally learns the t...   31-07-2020       8.439   \n",
      "2  Framed in the 1940s for the double murder of h...   23-09-1994      65.570   \n",
      "3  Spanning the years 1945 to 1955, a chronicle o...   14-03-1972      63.277   \n",
      "4  The final part of the film adaption of the ero...   19-11-2020      26.691   \n",
      "\n",
      "   vote_average  vote_count  \n",
      "0           8.7        2763  \n",
      "1           8.7        1223  \n",
      "2           8.7       18637  \n",
      "3           8.7       14052  \n",
      "4           8.7         773  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8551 entries, 0 to 8550\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Unnamed: 0    8551 non-null   int64  \n",
      " 1   id            8551 non-null   int64  \n",
      " 2   title         8551 non-null   object \n",
      " 3   overview      8531 non-null   object \n",
      " 4   release_date  8551 non-null   object \n",
      " 5   popularity    8551 non-null   float64\n",
      " 6   vote_average  8551 non-null   float64\n",
      " 7   vote_count    8551 non-null   int64  \n",
      "dtypes: float64(2), int64(3), object(3)\n",
      "memory usage: 534.6+ KB\n",
      "Dataset Info:\n",
      " None\n",
      "Dataset Statistics:\n",
      "         Unnamed: 0             id    popularity  vote_average    vote_count\n",
      "count  8551.000000    8551.000000   8551.000000   8551.000000   8551.000000\n",
      "mean   4275.000000  135552.118933     26.870217      6.578120   1496.749269\n",
      "std    2468.605409  182298.697711    166.606317      0.825626   2464.793441\n",
      "min       0.000000       5.000000      0.600000      2.200000    200.000000\n",
      "25%    2137.500000    9686.500000     10.049000      6.000000    320.000000\n",
      "50%    4275.000000   22584.000000     13.217000      6.600000    593.000000\n",
      "75%    6412.500000  261431.000000     20.771500      7.200000   1439.500000\n",
      "max    8550.000000  793723.000000  11701.435000      8.700000  28740.000000\n"
     ]
    }
   ],
   "source": [
    "## file_path=\"/Users/sarah/OneDrive/Desktop/New folder/movies.csv\"\n",
    "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Display initial information\n",
    "print(\"Initial Data Shape:\", df.shape)\n",
    "print(\"Column Names:\", df.columns)\n",
    "print(\"First 5 Rows:\\n\", df.head(5))\n",
    "print(\"Dataset Info:\\n\", df.info())\n",
    "print(\"Dataset Statistics:\\n\", df.describe())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92837d12-83d8-4673-b2f6-defde062bd9b",
   "metadata": {},
   "source": [
    "DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c9a90ad-2c9e-4c8b-85b1-ba1c85bc9259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data Shape: (8551, 7)\n",
      "Missing Values:\n",
      " id              0\n",
      "title           0\n",
      "overview        0\n",
      "release_date    0\n",
      "popularity      0\n",
      "vote_average    0\n",
      "vote_count      0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8551 entries, 0 to 8550\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   id            8551 non-null   int64         \n",
      " 1   title         8551 non-null   object        \n",
      " 2   overview      8551 non-null   object        \n",
      " 3   release_date  8551 non-null   datetime64[ns]\n",
      " 4   popularity    8551 non-null   float64       \n",
      " 5   vote_average  8551 non-null   float64       \n",
      " 6   vote_count    8551 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(2), int64(2), object(2)\n",
      "memory usage: 467.8+ KB\n",
      "Dataset Info:\n",
      " None\n",
      "Cleaned data saved to /Users/sarah/OneDrive/Desktop/New folder/movies_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"/Users/sarah/OneDrive/Desktop/New folder/movies.csv\"\n",
    "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# Fix missing values safely\n",
    "df.loc[:, \"overview\"] = df[\"overview\"].fillna(\"No overview available\")\n",
    "\n",
    "# Convert release_date to datetime (correcting day/month order)\n",
    "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Display final dataset info\n",
    "print(\"Cleaned Data Shape:\", df.shape)\n",
    "print(\"Missing Values:\\n\", df.isnull().sum())\n",
    "print(\"Dataset Info:\\n\", df.info())\n",
    "\n",
    "# Save cleaned data\n",
    "cleaned_file_path = \"/Users/sarah/OneDrive/Desktop/New folder/movies_cleaned.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "print(f\"Cleaned data saved to {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "436a6a34-6577-4a68-b0c2-5c347d5378cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data (First 5 Rows):\n",
      "        id                        title  \\\n",
      "0   19404  Dilwale Dulhania Le Jayenge   \n",
      "1  724089    Gabriel's Inferno Part II   \n",
      "2     278     The Shawshank Redemption   \n",
      "3     238                The Godfather   \n",
      "4  761053   Gabriel's Inferno Part III   \n",
      "\n",
      "                                            overview release_date  popularity  \\\n",
      "0  Raj is a rich, carefree, happy-go-lucky second...   1995-10-20      18.433   \n",
      "1  Professor Gabriel Emerson finally learns the t...   2020-07-31       8.439   \n",
      "2  Framed in the 1940s for the double murder of h...   1994-09-23      65.570   \n",
      "3  Spanning the years 1945 to 1955, a chronicle o...   1972-03-14      63.277   \n",
      "4  The final part of the film adaption of the ero...   2020-11-19      26.691   \n",
      "\n",
      "   vote_average  vote_count  \n",
      "0           8.7        2763  \n",
      "1           8.7        1223  \n",
      "2           8.7       18637  \n",
      "3           8.7       14052  \n",
      "4           8.7         773  \n",
      "Cleaned data saved to /Users/sarah/OneDrive/Desktop/New folder/movies_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/Users/sarah/OneDrive/Desktop/New folder/movies.csv\"\n",
    "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# Fill missing values\n",
    "df.loc[:, \"overview\"] = df[\"overview\"].fillna(\"No overview available\")\n",
    "\n",
    "# Convert release_date to datetime\n",
    "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Display cleaned data\n",
    "print(\"Cleaned Data (First 5 Rows):\\n\", df.head())\n",
    "\n",
    "\n",
    "# Save the cleaned data\n",
    "cleaned_file_path = \"/Users/sarah/OneDrive/Desktop/New folder/movies_cleaned.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "print(f\"Cleaned data saved to {cleaned_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8260156c-4a42-434c-8dbf-2cd63e64c178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and Tokenized Data (First 5 Rows):\n",
      "                          title  \\\n",
      "0  Dilwale Dulhania Le Jayenge   \n",
      "1    Gabriel's Inferno Part II   \n",
      "2     The Shawshank Redemption   \n",
      "3                The Godfather   \n",
      "4   Gabriel's Inferno Part III   \n",
      "\n",
      "                                            overview  \\\n",
      "0  Raj is a rich, carefree, happy-go-lucky second...   \n",
      "1  Professor Gabriel Emerson finally learns the t...   \n",
      "2  Framed in the 1940s for the double murder of h...   \n",
      "3  Spanning the years 1945 to 1955, a chronicle o...   \n",
      "4  The final part of the film adaption of the ero...   \n",
      "\n",
      "                                      overview_words  \\\n",
      "0  [Raj, is, a, rich, ,, carefree, ,, happy-go-lu...   \n",
      "1  [Professor, Gabriel, Emerson, finally, learns,...   \n",
      "2  [Framed, in, the, 1940s, for, the, double, mur...   \n",
      "3  [Spanning, the, years, 1945, to, 1955, ,, a, c...   \n",
      "4  [The, final, part, of, the, film, adaption, of...   \n",
      "\n",
      "                                  overview_sentences  \n",
      "0  [Raj is a rich, carefree, happy-go-lucky secon...  \n",
      "1  [Professor Gabriel Emerson finally learns the ...  \n",
      "2  [Framed in the 1940s for the double murder of ...  \n",
      "3  [Spanning the years 1945 to 1955, a chronicle ...  \n",
      "4  [The final part of the film adaption of the er...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download NLTK punkt package (only need to run once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example of tokenizing the 'overview' column into words and sentences\n",
    "# Tokenize 'overview' column into words\n",
    "df['overview_words'] = df['overview'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Tokenize 'overview' column into sentences\n",
    "df['overview_sentences'] = df['overview'].apply(lambda x: sent_tokenize(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(\"Cleaned and Tokenized Data (First 5 Rows):\\n\", df[['title', 'overview', 'overview_words', 'overview_sentences']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0338c189-c638-4444-8fa5-aabc5fea6846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and Tokenized Data (First 5 Rows):\n",
      "                          title  \\\n",
      "0  Dilwale Dulhania Le Jayenge   \n",
      "1    Gabriel's Inferno Part II   \n",
      "2     The Shawshank Redemption   \n",
      "3                The Godfather   \n",
      "4   Gabriel's Inferno Part III   \n",
      "\n",
      "                                            overview  \\\n",
      "0  Raj is a rich, carefree, happy-go-lucky second...   \n",
      "1  Professor Gabriel Emerson finally learns the t...   \n",
      "2  Framed in the 1940s for the double murder of h...   \n",
      "3  Spanning the years 1945 to 1955, a chronicle o...   \n",
      "4  The final part of the film adaption of the ero...   \n",
      "\n",
      "                                      overview_words  \\\n",
      "0  [Raj, is, a, rich, ,, carefree, ,, happy-go-lu...   \n",
      "1  [Professor, Gabriel, Emerson, finally, learns,...   \n",
      "2  [Framed, in, the, 1940s, for, the, double, mur...   \n",
      "3  [Spanning, the, years, 1945, to, 1955, ,, a, c...   \n",
      "4  [The, final, part, of, the, film, adaption, of...   \n",
      "\n",
      "                                  overview_sentences  \n",
      "0  [Raj is a rich, carefree, happy-go-lucky secon...  \n",
      "1  [Professor Gabriel Emerson finally learns the ...  \n",
      "2  [Framed in the 1940s for the double murder of ...  \n",
      "3  [Spanning the years 1945 to 1955, a chronicle ...  \n",
      "4  [The final part of the film adaption of the er...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download NLTK punkt package (only need to run once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example of tokenizing the 'overview' column into words and sentences\n",
    "# Tokenize 'overview' column into words\n",
    "df['overview_words'] = df['overview'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Tokenize 'overview' column into sentences\n",
    "df['overview_sentences'] = df['overview'].apply(lambda x: sent_tokenize(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(\"Cleaned and Tokenized Data (First 5 Rows):\\n\", df[['title', 'overview', 'overview_words', 'overview_sentences']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e2ec186d-62f8-4acb-842e-2702da726ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and Tokenized Data (First 5 Rows):\n",
      "                          title  \\\n",
      "0  Dilwale Dulhania Le Jayenge   \n",
      "1    Gabriel's Inferno Part II   \n",
      "2     The Shawshank Redemption   \n",
      "3                The Godfather   \n",
      "4   Gabriel's Inferno Part III   \n",
      "\n",
      "                                            overview  \\\n",
      "0  Raj is a rich, carefree, happy-go-lucky second...   \n",
      "1  Professor Gabriel Emerson finally learns the t...   \n",
      "2  Framed in the 1940s for the double murder of h...   \n",
      "3  Spanning the years 1945 to 1955, a chronicle o...   \n",
      "4  The final part of the film adaption of the ero...   \n",
      "\n",
      "                                      overview_words  \n",
      "0  [Raj, is, a, rich, ,, carefree, ,, happy-go-lu...  \n",
      "1  [Professor, Gabriel, Emerson, finally, learns,...  \n",
      "2  [Framed, in, the, 1940s, for, the, double, mur...  \n",
      "3  [Spanning, the, years, 1945, to, 1955, ,, a, c...  \n",
      "4  [The, final, part, of, the, film, adaption, of...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK punkt package (only need to run once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize 'overview' column into words\n",
    "df['overview_words'] = df['overview'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Display the first few rows of the tokenized data\n",
    "print(\"Cleaned and Tokenized Data (First 5 Rows):\\n\", df[['title', 'overview', 'overview_words']].head())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08ba96c2-dfbe-46e6-a65e-2b63e6589334",
   "metadata": {},
   "source": [
    "Find the words to their root form without considering the actual meaning of NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a9bf53b-05d5-484f-b594-a0e1a9918d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Sample:\n",
      "    Unnamed: 0      id                        title  \\\n",
      "0           0   19404  Dilwale Dulhania Le Jayenge   \n",
      "1           1  724089    Gabriel's Inferno Part II   \n",
      "2           2     278     The Shawshank Redemption   \n",
      "3           3     238                The Godfather   \n",
      "4           4  761053   Gabriel's Inferno Part III   \n",
      "\n",
      "                                            overview release_date  popularity  \\\n",
      "0  Raj is a rich, carefree, happy-go-lucky second...   20-10-1995      18.433   \n",
      "1  Professor Gabriel Emerson finally learns the t...   31-07-2020       8.439   \n",
      "2  Framed in the 1940s for the double murder of h...   23-09-1994      65.570   \n",
      "3  Spanning the years 1945 to 1955, a chronicle o...   14-03-1972      63.277   \n",
      "4  The final part of the film adaption of the ero...   19-11-2020      26.691   \n",
      "\n",
      "   vote_average  vote_count  \n",
      "0           8.7        2763  \n",
      "1           8.7        1223  \n",
      "2           8.7       18637  \n",
      "3           8.7       14052  \n",
      "4           8.7         773  \n",
      "Cleaned and Stemmed Data (First 5 Rows):\n",
      "                          title  \\\n",
      "0  Dilwale Dulhania Le Jayenge   \n",
      "1    Gabriel's Inferno Part II   \n",
      "2     The Shawshank Redemption   \n",
      "3                The Godfather   \n",
      "4   Gabriel's Inferno Part III   \n",
      "\n",
      "                                            overview  \\\n",
      "0  Raj is a rich, carefree, happy-go-lucky second...   \n",
      "1  Professor Gabriel Emerson finally learns the t...   \n",
      "2  Framed in the 1940s for the double murder of h...   \n",
      "3  Spanning the years 1945 to 1955, a chronicle o...   \n",
      "4  The final part of the film adaption of the ero...   \n",
      "\n",
      "                                    overview_stemmed  \n",
      "0  raj is a rich carefre happy-go-lucki second ge...  \n",
      "1  professor gabriel emerson final learn the trut...  \n",
      "2  frame in the 1940 for the doubl murder of hi w...  \n",
      "3  span the year 1945 to 1955 a chronicl of the f...  \n",
      "4  the final part of the film adapt of the erot r...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "# Download NLTK resources (only need to run once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Load your movie dataset CSV file\n",
    "file_path = \"/Users/sarah/OneDrive/Desktop/New folder/movies.csv\"  # Update with your file path\n",
    "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Check the first few rows of your dataset to confirm column names\n",
    "print(\"Initial Data Sample:\\n\", df.head())\n",
    "\n",
    "# Tokenize and stem the 'overview' column\n",
    "def stem_text(text):\n",
    "    if isinstance(text, str):  # Ensure the text is a string\n",
    "        words = word_tokenize(text)  # Tokenize the text into words\n",
    "        # Remove punctuation and stem each word\n",
    "        return ' '.join([ps.stem(word) for word in words if word not in string.punctuation])\n",
    "    else:\n",
    "        return \"\"  # Return empty string if text is not valid\n",
    "\n",
    "# Apply the stemming function to the 'overview' column\n",
    "df['overview_stemmed'] = df['overview'].apply(stem_text)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame with the stemmed column\n",
    "print(\"Cleaned and Stemmed Data (First 5 Rows):\\n\", df[['title', 'overview', 'overview_stemmed']].head())\n",
    "\n",
    "# Save the cleaned DataFrame with the stemmed overview to a new CSV (optional)\n",
    "df.to_csv(\"/Users/sarah/OneDrive/Desktop/New folder/movies_cleaned.csv\", index=False)  # Update with the desired output path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcb4ece4-9551-4cf2-bfc0-c3299809dbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4249b749-54a1-4585-8bab-dfd8424e3335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         title  \\\n",
      "0  Dilwale Dulhania Le Jayenge   \n",
      "1    Gabriel's Inferno Part II   \n",
      "2     The Shawshank Redemption   \n",
      "3                The Godfather   \n",
      "4   Gabriel's Inferno Part III   \n",
      "\n",
      "                                         POS_Tagging  \n",
      "0  [(Raj, NOUN), (is, VERB), (a, DET), (rich, ADJ...  \n",
      "1  [(Professor, NOUN), (Gabriel, NOUN), (Emerson,...  \n",
      "2  [(Framed, VERB), (in, ADP), (the, DET), (1940s...  \n",
      "3  [(Spanning, VERB), (the, DET), (years, NOUN), ...  \n",
      "4  [(The, DET), (final, ADJ), (part, NOUN), (of, ...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Download necessary corpora (can be skipped if already done)\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# Load your dataset (replace with the actual path to your file)\n",
    "file_path = \"/Users/sarah/OneDrive/Desktop/New folder/movies.csv\"\n",
    "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")  # Adjust encoding if necessary\n",
    "\n",
    "# Define a function to apply POS tagging using NLTK\n",
    "# Modify the pos_tagging function to handle non-string values\n",
    "def pos_tagging(text):\n",
    "    if isinstance(text, str):  # Ensure the text is a string\n",
    "        words = nltk.word_tokenize(text)  # Tokenizes the text\n",
    "        pos_tags = nltk.pos_tag(words, tagset='universal')  # POS tagging\n",
    "        return pos_tags\n",
    "    else:\n",
    "        return []  # Return an empty list for non-string values\n",
    "\n",
    "# Apply POS tagging to the 'overview' column\n",
    "df['POS_Tagging'] = df['overview'].apply(pos_tagging)\n",
    "\n",
    "# Print a sample of the DataFrame with POS tags\n",
    "print(df[['title', 'POS_Tagging']].head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9dc839ed-4165-4e6f-b223-3282a0b2d3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'id', 'title', 'overview', 'release_date', 'popularity',\n",
      "       'vote_average', 'vote_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print column names to check the correct one\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0e75b41-4a53-414c-8e2e-8c63408750fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         title  \\\n",
      "0  Dilwale Dulhania Le Jayenge   \n",
      "1    Gabriel's Inferno Part II   \n",
      "2     The Shawshank Redemption   \n",
      "3                The Godfather   \n",
      "4   Gabriel's Inferno Part III   \n",
      "\n",
      "                                         POS_Tagging  \n",
      "0  [(Raj, NNP), (rich, JJ), (,, ,), (carefree, JJ...  \n",
      "1  [(Professor, NNP), (Gabriel, NNP), (Emerson, N...  \n",
      "2  [(Framed, VBD), (1940s, CD), (double, JJ), (mu...  \n",
      "3  [(Spanning, VBG), (years, NNS), (1945, CD), (1...  \n",
      "4  [(final, JJ), (part, NN), (film, NN), (adaptio...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already installed\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Get English stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for POS tagging with stopword removal\n",
    "def pos_tagging(text):\n",
    "    if isinstance(text, str):  # Ensure the text is a string\n",
    "        words = nltk.word_tokenize(text)  # Tokenize the text\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]  # Remove stopwords\n",
    "        pos_tags = nltk.pos_tag(filtered_words)  # POS tagging\n",
    "        return pos_tags\n",
    "    else:\n",
    "        return []  # Return an empty list for non-string values\n",
    "\n",
    "# Apply POS tagging to the 'overview' column after removing stopwords\n",
    "df['POS_Tagging'] = df['overview'].fillna('').apply(pos_tagging)\n",
    "\n",
    "# Print a sample of the DataFrame with POS tags\n",
    "print(df[['title', 'POS_Tagging']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d649908d-9993-455a-b14e-97baefac825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function to perform POS tagging\n",
    "def pos_tagging(sentence):\n",
    "    words = nltk.word_tokenize(sentence)  # Tokenize the sentence\n",
    "    pos_tags = nltk.pos_tag(words)  # Assign POS tags\n",
    "    return pos_tags\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_result = pos_tagging(sentence)\n",
    "\n",
    "# Print POS-tagged words\n",
    "print(pos_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac849a83-a8cd-4a62-bfe4-1671193d8d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sarah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         title  \\\n",
      "0  Dilwale Dulhania Le Jayenge   \n",
      "1    Gabriel's Inferno Part II   \n",
      "2     The Shawshank Redemption   \n",
      "3                The Godfather   \n",
      "4   Gabriel's Inferno Part III   \n",
      "\n",
      "                                         POS_Tagging  \n",
      "0  [(Raj, NOUN), (rich, ADJ), (,, .), (carefree, ...  \n",
      "1  [(Professor, NOUN), (Gabriel, NOUN), (Emerson,...  \n",
      "2  [(Framed, VERB), (1940s, NUM), (double, ADJ), ...  \n",
      "3  [(Spanning, VERB), (years, NOUN), (1945, NUM),...  \n",
      "4  [(final, ADJ), (part, NOUN), (film, NOUN), (ad...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your dataset (replace with your actual dataset path)\n",
    "df = pd.read_csv(\"/Users/sarah/OneDrive/Desktop/New folder/movies.csv\")  # Make sure this file exists\n",
    "\n",
    "# Ensure the \"overview\" column exists\n",
    "if 'overview' not in df.columns:\n",
    "    raise KeyError(\"The 'overview' column is missing from the dataset!\")\n",
    "\n",
    "# Define a function for POS tagging with stopword removal\n",
    "def pos_tagging(text):\n",
    "    if isinstance(text, str):  # Check if text is a valid string\n",
    "        words = nltk.word_tokenize(text)  # Tokenize the text\n",
    "        filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]  # Remove stopwords\n",
    "        pos_tags = nltk.pos_tag(filtered_words, tagset='universal')  # Assign POS tags\n",
    "        return pos_tags\n",
    "    return None  # Handle missing values\n",
    "\n",
    "# Apply POS tagging to the \"overview\" column\n",
    "df['POS_Tagging'] = df['overview'].apply(pos_tagging)\n",
    "\n",
    "# Print a sample of the DataFrame\n",
    "print(df[['title', 'POS_Tagging']].head())\n",
    "\n",
    "# Save the output to a new CSV file\n",
    "df.to_csv(\"movie_dataset_with_pos.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c6bb835-ed31-42ae-a609-214f734905a5",
   "metadata": {},
   "source": [
    "REGULAR EXPRESSION PROBLEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "16b03481-fbc7-45a3-af9a-bae82ec7694e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available methods in re package: ['A', 'ASCII', 'DEBUG', 'DOTALL', 'I', 'IGNORECASE', 'L', 'LOCALE', 'M', 'MULTILINE', 'Match', 'NOFLAG', 'Pattern', 'RegexFlag', 'S', 'Scanner', 'T', 'TEMPLATE', 'U', 'UNICODE', 'VERBOSE', 'X', '_MAXCACHE', '_MAXCACHE2', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_cache', '_cache2', '_casefix', '_compile', '_compile_template', '_compiler', '_constants', '_parser', '_pickle', '_special_chars_map', '_sre', 'compile', 'copyreg', 'enum', 'error', 'escape', 'findall', 'finditer', 'fullmatch', 'functools', 'match', 'purge', 'search', 'split', 'sub', 'subn', 'template']\n",
      "['e', 'g', 'e', 'i', 'e', 'd', 'f', 'i', 'd', 'a', 'e', 'i', 'e']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "False\n",
      "False\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample text for testing\n",
    "text = \"Regex is used to find patterns in text.\"\n",
    "\n",
    "# 1. Find all available methods for the re package\n",
    "methods = dir(re)\n",
    "print(\"Available methods in re package:\", methods)\n",
    "\n",
    "# 2. Find all lowercase characters between 'a' and 'm'\n",
    "print(re.findall(r'[a-m]', text))\n",
    "\n",
    "# 3. Find all digit characters\n",
    "print(re.findall(r'\\d', text))\n",
    "\n",
    "# 4. Search for a sequence that starts with 'he', followed by two (any) characters, and an 'o'\n",
    "print(re.findall(r'he..o', text))\n",
    "\n",
    "# 5. Return anything that starts with 'he', followed by any characters, and an 'o'\n",
    "print(re.findall(r'he.*o', text))\n",
    "\n",
    "# 6. Check if the string starts with 'hello'\n",
    "print(bool(re.match(r'^hello', text)))\n",
    "\n",
    "# 7. Check if the string ends with 'world'\n",
    "print(bool(re.search(r'world$', text)))\n",
    "\n",
    "# 8. Check if the string contains 'ai' followed by 0 or more 'x' characters\n",
    "print(re.findall(r'aix*', text))\n",
    "\n",
    "# 9. Check if the string contains 'ai' followed by 1 or more 'x' characters\n",
    "print(re.findall(r'aix+', text))\n",
    "\n",
    "# 10. Check if the string contains 'a' followed by exactly two 'l' characters\n",
    "print(re.findall(r'all', text))\n",
    "\n",
    "# 11. Check if the string contains either 'falls' or 'stays'\n",
    "print(re.findall(r'falls|stays', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e613e681-47db-467d-a917-690f4d99a01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[]\n",
      "['ain', 'ain', 'ain']\n",
      "['ain', 'ain', 'ain', 'ain']\n",
      "['ain']\n",
      "True\n",
      "['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', ',', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 't', 'e', 's', 't', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', 'a', 'i', ',', ' ', 'a', 'x', ',', ' ', 'a', 'i', 'x', ',', ' ', 'a', 'n', 'd', ' ', 'n', 'u', 'm', 'b', 'e', 'r', 's', ' ', '!', ' ', 'T', 'h', 'e', ' ', 'r', 'a', 'i', 'n', ' ', 'i', 'n', ' ', 'S', 'p', 'a', 'i', 'n', ' ', 'f', 'a', 'l', 'l', 's', ' ', 'm', 'a', 'i', 'n', 'l', 'y', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'p', 'l', 'a', 'i', 'n', '.']\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd', ',', 't', 'h', 'i', 's', 'i', 's', 'a', 't', 'e', 's', 't', 's', 't', 'r', 'i', 'n', 'g', 'w', 'i', 't', 'h', 'a', 'i', ',', 'a', 'x', ',', 'a', 'i', 'x', ',', 'a', 'n', 'd', 'n', 'u', 'm', 'b', 'e', 'r', 's', '1', '2', '3', '4', '5', '!', 'T', 'h', 'e', 'r', 'a', 'i', 'n', 'i', 'n', 'S', 'p', 'a', 'i', 'n', 'f', 'a', 'l', 'l', 's', 'm', 'a', 'i', 'n', 'l', 'y', 'i', 'n', 't', 'h', 'e', 'p', 'l', 'a', 'i', 'n', '.']\n",
      "['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd', 't', 'h', 'i', 's', 'i', 's', 'a', 't', 'e', 's', 't', 's', 't', 'r', 'i', 'n', 'g', 'w', 'i', 't', 'h', 'a', 'i', 'a', 'x', 'a', 'i', 'x', 'a', 'n', 'd', 'n', 'u', 'm', 'b', 'e', 'r', 's', '1', '2', '3', '4', '5', 'T', 'h', 'e', 'r', 'a', 'i', 'n', 'i', 'n', 'S', 'p', 'a', 'i', 'n', 'f', 'a', 'l', 'l', 's', 'm', 'a', 'i', 'n', 'l', 'y', 'i', 'n', 't', 'h', 'e', 'p', 'l', 'a', 'i', 'n']\n",
      "[' ', ',', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ',', ' ', ',', ' ', ',', ' ', ' ', ' ', '!', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '.']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 1. Check if the string starts with 'The'\n",
    "print(bool(re.match(r'^The', text)))\n",
    "\n",
    "# 2. Check if 'ain' is present at the beginning of a WORD\n",
    "print(re.findall(r'\\bain', text))\n",
    "\n",
    "# 3. Check if 'ain' is present at the end of a WORD\n",
    "print(re.findall(r'ain\\b', text))\n",
    "\n",
    "# 4. Check if 'ain' is present, but NOT at the beginning of a word\n",
    "print(re.findall(r'\\Bain', text))\n",
    "\n",
    "# 5. Check if 'ain' is present, but NOT at the end of a word\n",
    "print(re.findall(r'ain\\B', text))\n",
    "\n",
    "# 6. Check if the string contains any digits (0-9)\n",
    "print(bool(re.search(r'\\d', text)))\n",
    "\n",
    "# 7. Return a match at every non-digit character\n",
    "print(re.findall(r'\\D', text))\n",
    "\n",
    "# 8. Return a match at every white-space character\n",
    "print(re.findall(r'\\s', text))\n",
    "\n",
    "# 9. Return a match at every NON white-space character\n",
    "print(re.findall(r'\\S', text))\n",
    "\n",
    "# 10. Return a match at every word character (a-z, A-Z, 0-9, _)\n",
    "print(re.findall(r'\\w', text))\n",
    "\n",
    "# 11. Return a match at every NON word character (!, ?, space, etc.)\n",
    "print(re.findall(r'\\W', text))\n",
    "\n",
    "# 12. Check if the string ends with 'Spain'\n",
    "print(bool(re.search(r'Spain$', text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "07823590-1315-4945-aa88-40c1a876b839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r', 'a', 'r', 'n', 'a', 'a', 'a', 'a', 'n', 'n', 'r', 'r', 'a', 'n', 'n', 'a', 'n', 'a', 'a', 'n', 'n', 'a', 'n']\n",
      "['h', 'e', 'l', 'l', 'l', 'd', 'h', 'i', 'i', 'a', 'e', 'i', 'n', 'g', 'i', 'h', 'a', 'i', 'a', 'a', 'i', 'a', 'n', 'd', 'n', 'm', 'b', 'e', 'h', 'e', 'a', 'i', 'n', 'i', 'n', 'a', 'i', 'n', 'f', 'a', 'l', 'l', 'm', 'a', 'i', 'n', 'l', 'i', 'n', 'h', 'e', 'l', 'a', 'i', 'n']\n",
      "['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'l', 'd', ',', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', ' ', 't', 'e', 's', 't', ' ', 's', 't', 'i', 'g', ' ', 'w', 'i', 't', 'h', ' ', 'i', ',', ' ', 'x', ',', ' ', 'i', 'x', ',', ' ', 'd', ' ', 'u', 'm', 'b', 'e', 's', ' ', '1', '2', '3', '4', '5', '!', ' ', 'T', 'h', 'e', ' ', 'i', ' ', 'i', ' ', 'S', 'p', 'i', ' ', 'f', 'l', 'l', 's', ' ', 'm', 'i', 'l', 'y', ' ', 'i', ' ', 't', 'h', 'e', ' ', 'p', 'l', 'i', '.']\n",
      "['1', '2', '3']\n",
      "['1', '2', '3', '4', '5']\n",
      "['12', '34']\n",
      "['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd', 't', 'h', 'i', 's', 'i', 's', 'a', 't', 'e', 's', 't', 's', 't', 'r', 'i', 'n', 'g', 'w', 'i', 't', 'h', 'a', 'i', 'a', 'x', 'a', 'i', 'x', 'a', 'n', 'd', 'n', 'u', 'm', 'b', 'e', 'r', 's', 'T', 'h', 'e', 'r', 'a', 'i', 'n', 'i', 'n', 'S', 'p', 'a', 'i', 'n', 'f', 'a', 'l', 'l', 's', 'm', 'a', 'i', 'n', 'l', 'y', 'i', 'n', 't', 'h', 'e', 'p', 'l', 'a', 'i', 'n']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Check if the string has any a, r, or n characters\n",
    "print(re.findall(r'[arn]', text))\n",
    "\n",
    "# 2. Check if the string has any characters between a and n\n",
    "print(re.findall(r'[a-n]', text))\n",
    "\n",
    "# 3. Check if the string has other characters than a, r, or n\n",
    "print(re.findall(r'[^arn]', text))\n",
    "\n",
    "# 4. Check if the string has any 0, 1, 2, or 3 digits\n",
    "print(re.findall(r'[0123]', text))\n",
    "\n",
    "# 5. Check if the string has any digits\n",
    "print(re.findall(r'\\d', text))\n",
    "\n",
    "# 6. Check if the string has any two-digit numbers, from 00 to 59\n",
    "print(re.findall(r'[0-5][0-9]', text))\n",
    "\n",
    "# 7. Check if the string has any characters from a to z lower case, and A to Z upper case\n",
    "print(re.findall(r'[a-zA-Z]', text))\n",
    "\n",
    "# 8. Check if the string has any '+' characters\n",
    "print(re.findall(r'\\+', text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
